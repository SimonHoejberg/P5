\section{Intelligent Agents}
 
As \namep is an autonomous turret and needs to be able to hit a moving target,
it needs to have some intelligence in order to determine how to position itself
to hit the target. The turret can be considered intelligent
\citep[ch.1.1]{MIBook} when its decisions are explainable by computations and
its intelligence can be judged by:

\begin{enumerate}
  \item If what it does is appropriate to its circumstances and goals.
  \item It is flexible to changing environments and changing goals.
  \item It learns from experience.
  \item It makes appropriate choices given its perceptual and computational
  limits.
\end{enumerate}
%See MI book, chapter 1.1 p.4

In order to be intelligent \name needs to be able to represent its knowledge in
a suitable way that allows it to work it into a suitable solution. 

\subsection{Knowledge Representation}\label{KR}
%See MI book chapter 1.4 p.12

As \name is a turret it needs to be able to convert the problem of hitting a
moving target into a suitable representation. This representation can then be
computed into a suitable output which in turn can be interpreted as
a solution, presumably whether or not the target is hit. This can be seen in
\autoref{MIRep}.

\figx[0.8]{MIRep}{An illustration of how a problem is converted into a solution
\citep[Chap. 1.4]{MIBook}}

The representation is a piece of specific knowledge from the representation
scheme. The representation scheme is used to make sure the individual
representations are detailed and close to the problem, such that the necessary
knowledge can be used to solve said problem \citep{MIBook}. Some examples of the
knowledge that \name could have is: the targets speed, its direction and
knowledge of where the turret itself is pointing. When the turret has a
sufficient amount of knowledge, it should be used in a way that makes it
possible to compute an output. Whether this output is a solution or not, depends
on what a sufficient solution is. Generally there are 4 different categories of
solutions:

\begin{itemize}
  \item \textbf{Optimal solution} - The best possible solution to the problem.
  \item \textbf{Satisficing solution} - A solution that is both satisfying and
  sufficient.
  \item \textbf{Approximately optimal solution} - A solution that is close to
  the optimal solution.
  \item \textbf{Probable solution} - A solution that is likely to be a solution
  but is not guaranteed to be.
\end{itemize}

As \name is shooting on a moving target. The solution needs to reliably be able
to hit the target, but as it is a real-time system there are certain limits to
how much time can be spent searching and waiting for the optimal shot.

\subsection{Design Space}
The design space of an intelligent agent can be considered a series of 8
factors, which are used to give an estimation of the system \citep[Chap
1.5]{MIBook}. These factors are: modularity, representation scheme, planning
horizon, uncertainty, preference, number of agents, learning and computational
limits.

\subsubsection{Modularity}
Modularity describes how easily the system can be divided into modules, as well
as how easy it is to understand these modules separately. Modularity has three
different levels, flat, modular and hierachical. Flat means that everything is
contained within a single module. Modular means that the system can be divided
into several modules that can be understood separately from the system.
Hierachical means that the modules can recursively have additional modules
inside them. \name as an autonomous turret needs to have several functions, for
example it needs to be able to turn, aim and determine the position of a target.
This means that \name should be somewhat modular due to the fact that it has
different tasks, that can each be considred modules of their own.

\subsubsection{Representation Scheme}
As mentioned in \autoref{KR} the representation scheme is used to house the
various knowledge the agent could need to perform. This knowledge is usually
represented in terms of states. A state is a specific combination of
the knowledge. For example a turret with the ability to turn 180 degrees and
tilt its cannon 45 degrees would have $180 * 45$ states, a specific state
would have the turret turned 90 degrees and the cannon tilted at an angle of 42
degrees. This leads to alot of different states as the amount of possible values
increases. Instead of reasoning using states it is possible to combine the
relevant values into features. This means that the state is determined by the
value of the features, such as a feature, Tilt, could cover how tilted the
cannon is and have the values covering 0 to 45.

\subsubsection{Planning Horizon}
The planning horizon of an intelligent agent is the time it considers the
consequences of its actions. For example a child might only do something if
there is an immediate reward, whereas an adult might have the foresight to do
something for a reward thats some time off. There exist the following types of planning
horizons:
\begin{itemize}
	\item \textbf{Non-planning} - The agent does not consider the future when
	making decisions, or time is not involved as a factor.
	\item \textbf{Finite horizon} - The agent considers the future for a fixed
	finite set of steps. 
	\item \textbf{Indefinite horizon} - The agent looks ahead for finite but not
	fixed number of steps.
	\item \textbf{Infinite horizon} - The agent plans on running forever.	
\end{itemize}

Considering \name is an autonomous turret, non-planning can already be ruled
out as that would that would result in a naive approach for the turret where it
would fire when it sees a target.

\subsubsection{Uncertainty}

There are two parts to uncertainty. The first part is sensing uncertainty while
the second is effect uncertainty. Sensing uncertainty concerns what the agent
can determine based on its observations. There are two possibilities concerning
what it can sense:
\begin{itemize}
  \item \textbf{Fully observable} - The agent can determine the current state
  based on observations.
  \item \textbf{Partially observable} - The agent can not directly observe the
  current state. This can be caused by noisy observations or when multiple
  states result in the same observations.
\end{itemize}

In order have knowledge of the target, \name will need to observe since the
properties of the target are not known by \name beforehand. This results in
uncertainty as we not only need to predict the future position of the target,
the sensors may provide noisy observations. We can thus already determine that
\name will only be able to partially observe the state.\nl

Effect uncertainty concerns how accuratly the agent can determine the resulting
state based on an action in its current state. There are two possibilities:
\begin{itemize}
  \item \textbf{Deterministic} - The resulting state can be determined based on
  an action and the current state.
  \item \textbf{Stochastic} - The resulting state is the result of a probability
  distribution.
\end{itemize}

There are certain deterministic aspects in \name as it is able to reach a
firing state when it has received information about the target, without
resorting to probability.
 
\subsubsection{Preferences}
For an agent to be able to make good decisions it is necessary to provide an
idea of the goals we are trying to accomplish. Furthermore there are certain
preferences concerning the trade-off between different solutions.

\begin{itemize}
  \item \textbf{Goals} - There are two types of goals, achievement goals and
  maintenance goals. Maintenance goals must be achieved in all the states it
  visits. Achievement goals should hold true for the final state. 
  \item \textbf{Complex preferences} - There are two types of preferences,
  ordinal preferences and cardinal preferences. Ordinal preferences describe
  preferences where only the order of the preferences indicate importance.
  Cardinal preferences are those that are prioritized according to their value,
  such that more value is better.
\end{itemize}

For this project, first and foremost an achievement goal is needed in order to
describe the goal for the agent. This is simply the combined state of features
that allows us to hit a target in motion. 

\subsubsection{Number of Agents}
Depending on what an agent needs to accomplish, it might need to take the
reasoning of other agents into account. This leads to two types of reasoning:
\begin{itemize}
  \item \textbf{Single agent reasoning} - The agent does not take other
  agents into account and assumes they are part of the environment.
  \item \textbf{Multiple agent reasoning} - The agent takes the reasoning of
  other agents into account.
\end{itemize}

In regards to \namep, it needs to take the reasoning of the target into
consideration in order to actually hit it.

% Reasoning with multiple agents can be hard. This is due to the fact that an
% agent must take into account the other agents goals and preferences and reason
% strategically about these. It can also be advantageous to ignore the other
% agents if their behavior is not impacted by what the agent does.
% \namep uses only a single agent due to the fact that their only is a single
% agent so it does not need to take any other agents into account.

\subsubsection{Learning}

There are two ways of handling knowledge: Either knowledge is given or
it is learned. 
\begin{itemize}
  \item \textbf{Knowledge is given} - The agent possesses a model which the
designer thinks is suitable for the task.
  \item \textbf{Knowledge is learned} - The agent can use data or past
  experiences to find a model that fits the data.
\end{itemize}

\name should make use of a given model, as it easy to determine and
calculate the various parts needed for projectile motion, see
\autoref{ProjMotion}.


% when it comes to learning it is the ability to improve
% behavior based on experience. A way to do this is with supervised learning where
% a set of input features and target features are given together with a set of
% traing examples, where the values for both input and output are given and a set
% of test examples where only input is given. The goal is to predict the target
% features for the test set as future examples.


\subsubsection{Computational Limits}
An angent may not always be able to compute the best action to a problem fast
enough, which could be due to memory limitation or just the sheer size of the
problem. This means that the agent may need to trade off the time it needs to
compute a solution with how good the solution is. This limit determines whether
the agent has:

\begin{itemize}
  \item Perfect rationality - The agent tries to find the best action without
  taking into account the limited resources.
  \item Bounded rationality - the agent decides the best ation given the
  computational limits.
\end{itemize}

\namep has bounded rationality due to the fact that the NXT does not have
unlimited resources or time. This means that in some situations, like deciding
when to shoot, it will have to determine the best course of action at the
moment instead of spending a lot of time computing the best solution.

\subsubsection{Belief Network}
A belief network is an acyclic directed graph consisting of random variables and
a set of conditional probalility distributions. This an be used to model a
dynamic system by treating a feature at a given time as a random variable, which
is the idea behind Markov chains. A markov chain is a special belief network
that represents sequences of values.
In a markov chain at state $S_t$ the future is conditionally independent from
the past given the present. This gives that if the transition probabilities are
the same for each time point. To specify a Markov chain two conditional
probabilities must be specified:
\begin{itemize}
  \item $p(S_0)$ - species the initial condition
  \item $P(S_{t+1}|S_t)$ - specifies the dynamics, which is the same for each $t
  \geq 0$
\end{itemize}

These kinds of chains are of interrest due to the fact that the stationary
assumption is a natural model because the dynamics of the world ussually does
not change. The network can also extend indefinetly which means that with a
small number of parameters the network can be infinite.\nl

The Markov chain can be augmented to include observations which makes it a
Hidden Markov model. Like the state transitions from a Markov chain the Hidden
Markov model also includes observations about the states. If more than one state
maps to the same observation it is partial and if the same state maps to
different observations at different times it is noisy. The observations are
modeled using $O_t$ for each time t whose domain is the set of possible
observations. A Hidden Markov model needs three conditional proballity
distributions
\begin{itemize}
  \item $P(S_0)$ - species the initial condition
  \item $P(S_{t+1}|S_t)$ - specifies the dynamics, which is the same for each $t
  \geq 0$
  \item $P(O_t|S_t)$ - specifies the sensor model
\end{itemize}

This model makes it possible to determine the current state based on previous
and current observations. This is useful when tracking the target in order to
adjust \namep accordingly. This is done by having a belief network with 3
variables $Loc_i$, $Obs_i$ and $Act_i$ see \autoref{BeliefNetwork}. $Loc_i$ for
each time i represents \name's poition at time i, $Obs_i$ for each time i reprensts the observation
made at time i and for each time i $act_i$ represents the action taken at time
i. The model assumes the that at time i \namep is in position $Loc_i$, it
observes $Obs_i$ and acts, it observes $Act_i$ and time progresses to i+1 where
\namep is in position $Loc_{i+1}$. This means that at any given time t the
observation is only dependent on the state and at time t+1 the position is
dependent on the position and action at time t. This gives that the position
at time t+1 is conditionally independent from previous positions, actions and
observations given the position and ation at time t. 

\figx{BeliefNetwork}{A belief network for position of \name}

%Although a HMM representation only shows few stages it can proceed
% indefinitely.
