\section{Intelligent Agents}

As \name is an autonomous turret and needs to be able to hit a moving target,
it needs to have some intelligence in order to determine how best to hit the
target. The turret can be considered intelligent when its decisions
are explainable by computations while its intelligence can be judged by:

\begin{enumerate}
  \item If what it does is appropriate to its circumstances and goals.
  \item It is flexible to changing environments and changing goals.
  \item It learns from experience.
  \item It makes appropriate choices given its perceptual and computational
  limits.
\end{enumerate}
%See MI book, chapter 1.1 p.4

In order to be intelligent \name needs to be able to represent its knowledge in
a suitable way that allows it to work it into a suitable solution.

\subsection{Knowledge Representation}\label{KR}
%See MI book chapter 1.4 p.12

As \name is a turret it needs to be able to convert the problem of hitting a
moving target into a suitable representation. The representation can then be
computed into a suitable output, which can be interpreted as a solution.

%(Consider using fig1.4 p12)

The representation is part of a representation scheme, which specifies the form
of the knowledge. In the case of \name it would need to represent the knowledge
about the movement of the target, the vertical angle of the canon and the
horisontal angle of the turret in a way that makes it possible to compute an
output. Whether this output is a solution or not, depends on what a sufficient
solution is. Generally there are 4 different categories of solutions:

\begin{itemize}
  \item Optimal solution - The best possible solution to the problem.
  \item Satisficing solution - A solution that is both satisfying and
  sufficient.
  \item Approximately optimal solution - A solution that is close to the optimal
  solution.
  \item Probable solution - A solution that is likely to be a solution but is
  not guaranteed to be.
\end{itemize}

As \name is shooting on a moving target. The solution needs to be reliable able
to hit the target, but as it is a real-time system there are certain limits to
how much time can be spent searching and waiting for the optimal shot.
\fix{Do we need to discuss which solution type we use?}{}


\subsection{Design Space}
The design space is defined by a series of 8 factors, which are used to give an
estimation of the system. These factors are:

\begin{itemize}
   \item Modularity
   \item Representation scheme
   \item Planning horizon
   \item Uncertainty
   \item Preference
   \item Number of agents
   \item Learning
   \item Computational limits
\end{itemize}

\subsubsection{Modularity}
Modularity describes how easily the system can be divided into modules, as well
as how easy it is to understand these modules separately. Modularity has three
different levels, flat, modular and hierachical. Flat means that everything is
contained within a single module. Modular means that the system can be divided
into several modules that can be understood separately from the system.
Hierachical means that the modules can recursively have additional modules
inside them. \name as an autonomous turret needs to have several separate
functions that allows for it turn, aim and determine the position of a target.
This means that \name should be somewhat modular due to the fact that it has
different tasks that can each be a module of its own.

\subsubsection{Representation Scheme}
As mentioned in \autoref{KR} the representation scheme is how knowledge is
represented. This knowledge is usually represented in terms of states which are
combination of all the relevant values. This leads to alot of different states
as the amount of possible values increases. Instead of reasoning using states it
is possible to combine the relevant values into feature. This means that the
state is determined by the value of the features and not the other way around.

The planning horizon of an intelligent agent is the time it considers the consequenses of its actions.
For example a little boy might only do something if the reward is somewhat
immediate(you have to get good grades so you can get a good job in the future),
 whereas an adult might have the foresight to do something, although the reward
 is further down the future(if i do well now, i might get a promotion in a few
 months). Thus we are talking about different planning horizons.

There exist the following types of planning horizons
\begin{itemize}
	\item Non-planning - If the system doesn’t consider the future when making decisions, or if time isn’t involved a factor
	\item Finite horizon - The agent considers the future for a fixed finite set of steps. If the agent only looks one step ahead, we call it a \textbf{greedy} or \textbf{myopic}
	\item Indefinite horizon - The agent looks ahead in a finite but not fixed number of steps. Consider the Google Maps directions feature - It only knows the destination, but doesn’t know how many steps it will take to complete the route
	\item Infinite horizon - The agent should run forever	
\end{itemize}

Considering our project of an autonomous turret, we can already rule out the non-planning horizon, since it will have to consider the future in order to make a decision.

On the other hand since our turret is an embedded system with limited computational power and memory, an infinite horizon isn’t realistic since the resulting statespace would require far too much computational power.

At this point in time it would be too early to predict what type of remaining horizon is most effective, however a finite horizon seems to accomplish what it is we’re trying to do.




% -------something about states---------
% ??? - maybe or maybe not
%
% --------Knowledge base (s1.3)---------
% AI is about reasoning
% An agent is a coupling of perception, reasoning and acting.
%
% Its actions depend on:
% - prior knowledge about the agent and the environment
% - history of interaction with the environment, which is composed of:
% + observation of the current environents
% + past experiences of previous actions and observations or other data, from
% which it can learn
% - goals that it must try to achieve or preferences over states of the world
% - Abilites which are the primitive actions it is capable of carrying out
% (consider using fig1.3 p11)
%
% Problems do you usually not have clear solutions
% To solve a problem a designer must:
% - flesh out the task and determine what constitutes a solution
% - represent the problem in a language with which a computer can reason
% - use the computer to compute an output, which is an answer presented to a user
% or a sequence
% (Consider using fig1.4 p12)
%
% A knowledge base is the representation of all of the knowledge stored on the
% agent. A representation should be:
% - rich enough to express the knowledge needed to solve the problem
% - as close to the problem as possible
% - amenable to efficient computation
% - able to be acquired from people, data and past experience
%
% Questions needed to be considered when given a problem:
% - what is a solution, and how good must a solution be?
% - how can a problem be represented?
% - how can the agent compute an output that can be interpreted as a solution to
% the problem? what about worst and average cases?
%
% Much work in AI is motivated by commonsense reasoning
%
% there are 4 common classes of solutions, they are not exclusive:
% - optimal solutions
% - satisficing solution --> satisfying + sufficient
% - approximate optimal solutions
% - probable solution
%
%  -------- Dimensions of complexity ---------
% Modularity:
% flat - modular - hierachical
%
% Representation scheme:
% States - features - relations -propositition
%
% Planning Horizon:
% non-planning - finite horizon - indefinite horizon - infinite horizon
%
% Uncertainty:
% - Sensing uncertainty:
% + Fully observable
% + Partially observable
%
% - Effect uncertainty:
% + Deterministic
% + Stochastic
%
% Preference:
% - goals are either achivement goals or maintenance goals
% - complex preferences, ordinal preference, cardinal preference
%
% Number of agents:
% - single agent
% - multiple agents
%
% Learning:
% - Knowledge is given
% - knowledge is learned
%
% Computational limits:
% - Perfect rationality
% - bounded rationality
%
% ------- Reasoning under uncertainty (ch6)-------
% propability as a measure of belief is Bayesian propability/subjective
% propability.
%
%



% -------What is an agent (S1.1 p4) --------
%
% An agent is something that acts in an environment.
% An agent acts intelligent when:
% - what it does is appropriate for its circumstances and its goals
% - it is flexible to changing environments and changing goals
% - it learns from experience
% - it makes appropriate choices given its perceptual and computational
% limitation. It typically cannot observe the state of the world and has finite
% memory and limited time.
