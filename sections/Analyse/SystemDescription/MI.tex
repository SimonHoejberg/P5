\section{Intelligent Agents}
 
As \name is an autonomous turret and needs to be able to hit a moving target.
It needs to have some intelligence in order to determine how to position itself
to hit the target. The turret can be considered intelligent when its decisions
are explainable by computations. How intelligent it is, can as
\citep[ch.1.1,p.4]{MIBook} states, be judged by:

\begin{enumerate}
  
  \item \say{What it does is appropriate to its circumstances and goals.}
  \item \say{It is flexible to changing environments and changing goals.}
  \item \say{It learns from experience.}
  \item \say{It makes appropriate choices given its perceptual and
  computational limits.}
\end{enumerate}
%See MI book, chapter 1.1 p.4

In order to be intelligent \name needs to be able to represent its
knowledge in a suitable way that allows it to work it into a suitable solution. 

\subsection{Knowledge Representation}\label{KR}
%See MI book chapter 1.4 p.12

In order to be able to intelligently control the turret, \name needs to be able
to model the problem of hitting a moving target into a suitable representation,
which can be understood by the computer. By modelling the problem into a
representation on which computational operations can be executed, it is possible
to use a computer to determine a solution. The result of this computation is a
suitable output which in turn can be interpreted as a solution, whether or not
the target is actually hit. The model below in \autoref{MIRep}, presents the
correlation between the informal and formal representation, where the informal
is models the problem domain, and the formal the computational domain.

\figxs[0.8]{MIRep}{An illustration of how a problem is converted into a
solution}{\citep[ch. 1.4 p.12]{MIBook}}

% The representation is a piece of knowledge from the representation
% scheme. 
When modelling a problem, we make use of knowledge as a means of choosing the
right solution. In the field of machine intelligence, knowledge is defined as
the information about a domain, which can be used to solve problems in this
domain. When implementing knowledge into a system, it will take the form of a
representation scheme, which defines the computational form this knowledge will
be represented by. The representation scheme is used to make sure the individual
representations are detailed and close to the problem, such that the necessary
knowledge can be used to solve said problem \citep[ch.1.4]{MIBook}.
Examples of knowledge that \name has is: the targets speed and how much the turret has
turned. When the turret has a sufficient amount of knowledge, it should be used
in a way that makes it possible to compute an output. Whether this output is a
solution or not, depends on what a solution is. For this purpose, there are 4
different categories of solutions \citep[ch.1.4.1]{MIBook}:

\begin{itemize}
  \item \textbf{Optimal solution} - A solution which has been deemed optimal
  through some measurement of quality.
  \item \textbf{Satisficing solution} - A solution that is both deemed
  satisfying by the end user and sufficient in terms of overall requirements. 
  \item \textbf{Approximately optimal solution} - A solution that is close in
  quality to the optimal solution. While not being perfect, this solution is
  deemed to be sufficient.
  \item \textbf{Probable solution} - A solution that is likely to be a solution
  but is not always guaranteed to be.
\end{itemize}

As \name is shooting on a moving target, the solution needs to be able
to  reliably hit the target. \name is also a real-time system, as such there are
certain limits to how much time can be spent searching and waiting for the
optimal shot.

\subsection{Design Space}
When designing intelligent agents, we have a number of design dimensions which
form a coarse framework for the design process. This framework represents the
design space of an intelligent agent, and can be considered a series of 8
factors, which are used to give an estimation of the system
\citep[ch.1.5]{MIBook}. These factors are: modularity, representation scheme,
planning horizon, uncertainty, preference, number of agents, learning and
computational limits.

\subsubsection{Modularity}
Modularity describes how easily the system can be divided into modules, as well
as how easy it is to understand these modules separately. Modularity has three
different levels, flat, modular and hierachical. Flat means that everything is
contained within a single module. Modular means that the system can be divided
into several modules that can be understood separately from the system.
Hierachical means that the modules can recursively have additional modules
inside them \citep[ch.1.5.1]{MIBook}. \name as an autonomous turret needs to
have several functions, for example it needs to be able to turn, aim and determine the position of a target.
This means that \name should be a modular system because, while it is a
relatively simple system, it has distinct parts which can each be considred
modules of their own.

\subsubsection{Representation Scheme}
As mentioned in \autoref{KR} the representation scheme is used to house the
various knowledge the agent could need to perform. This knowledge is usually
represented in terms of states. A state is a specific combination of
the knowledge \citep[ch.1.5.2]{MIBook}. For example a turret with the ability
to turn 180 degrees and tilt its cannon 45 degrees would have $180 * 45$ states. As an example, a
specific state would have the turret turned 90 degrees and the cannon tilted at an angle of 42
degrees. This leads to alot of different states as the amount of possible values
increases. Instead of reasoning using states it is possible to combine the
relevant values into features. This means that the state is determined by the
value of the features, such as a feature, Tilt, could cover how tilted the
cannon is and have the values covering 0 to 45.

\subsubsection{Planning Horizon}
The planning horizon for an intelligent agent governs how far ahead it plans
when it comes to the consequences of its actions. For example a child might
only do something if there is an immediate reward, whereas an adult might have
the foresight to do something for a reward thats some time off. There exist the
following types of planning horizons \citep[ch.1.5.3]{MIBook}:
\begin{itemize}
	\item \textbf{Non-planning} - The agent either does not consider the
	future when making decisions, or doesn't use time as a factor.
	\item \textbf{Finite horizon} - The agent considers the future for a fixed
	finite set of steps. 
	\item \textbf{Indefinite horizon} - The agent considers the future for a
	finite but not fixed number of steps.
	\item \textbf{Infinite horizon} - The agent plans on running forever.	
\end{itemize}

Considering \name is an autonomous turret, non-planning can already be ruled
out as that would that would result in a naive approach for the turret where it
would fire when it sees a target. Given the limited time a target is in range,
the turret can be considered having a finite horizon as the number of steps it
needs are limited.

\subsubsection{Uncertainty}
There are two parts to uncertainty. The first part is sensing uncertainty while
the second is effect uncertainty. Sensing uncertainty concerns what the agent
can determine based on its observations. There are two possibilities concerning
what it can sense \citep[ch.1.5.4]{MIBook}:
\begin{itemize}
  \item \textbf{Fully observable} - The agent can determine the current state
  based on observations.
  \item \textbf{Partially observable} - The agent can not directly observe
  the current state. This can be caused by noisy observations or when multiple
  states result in the same observations.
\end{itemize}

In order to have knowledge of the target, \name will need to observe since the
properties of the target are not known by \name beforehand. This results in
uncertainty as we not only need to predict the future position of the target,
the sensors may provide noisy observations. We can thus already determine that
\name will only be able to partially observe the state.\nl

Effect uncertainty concerns how accurately the agent can determine the resulting
state based on an action in its current state. There are two possibilities
\citep[ch.1.5.4]{MIBook}:
\begin{itemize}
  \item \textbf{Deterministic} - The resulting state can be determined based on
  an action and the current state.
  \item \textbf{Stochastic} - The resulting state is the result of a probability
  distribution.
\end{itemize}

\name can not directly observe the current state, it is only able
to observe some of it through its sensors, as such the system is stochastic.
Although the system is stochastic, it can be modelled as a deterministic system, where
the actions can be be detemined by what it can observe.
 
\subsubsection{Preferences}
For an agent to be able to make good decisions it is necessary to provide an
idea of the goals we are trying to accomplish. Furthermore there are certain
preferences concerning the trade-off between different solutions
\citep[ch.1.5.5]{MIBook}:

\begin{itemize}
  \item \textbf{Goals} - There are two types of goals, achievement goals and
  maintenance goals. Maintenance goals must be achieved in all the states it
  visits. Achievement goals should hold true for the final state. 
  \item \textbf{Complex preferences} - There are two types of preferences,
  ordinal preferences and cardinal preferences. Ordinal preferences describe
  preferences where only the order of the preferences indicate importance.
  Cardinal preferences are those that are prioritized according to their value,
  such that more value is better.
\end{itemize}

For this project, first and foremost an achievement goal is needed in order to
describe the goal for the agent. This is simply the combined state of features
that allows us to hit a target in motion. 

\subsubsection{Number of Agents}
Depending on what an agent needs to accomplish, it might need to take the
reasoning of other agents into account. This leads to two types of reasoning
\citep[ch.1.5.6]{MIBook}:
\begin{itemize}
  \item \textbf{Single agent reasoning} - The agent does not take other
  agents into account and assumes they are part of the environment.
  \item \textbf{Multiple agent reasoning} - The agents reasoning is also based
  on the other agents reasoning.
\end{itemize}

In regards to \namep, it needs to take the reasoning of the target into
consideration in order to actually hit it.

% Reasoning with multiple agents can be hard. This is due to the fact that an
% agent must take into account the other agents goals and preferences and reason
% strategically about these. It can also be advantageous to ignore the other
% agents if their behavior is not impacted by what the agent does.
% \namep uses only a single agent due to the fact that their only is a single
% agent so it does not need to take any other agents into account.

\subsubsection{Learning}
There are two ways of handling knowledge: knowledge can given in form of a
model or the agent can be given data to learn from \citep[ch.1.5.7]{MIBook}. 
\begin{itemize}
  \item \textbf{Knowledge is given} - The agent possesses a model which the
designer thinks is suitable for the task.
  \item \textbf{Knowledge is learned} - The agent can use data or past
  experiences to find a model that fits the data.
\end{itemize}

\name should make use of a prebuilt model that can accurately help it decide
what to do and calculate the various parts needed for projectile motion, see
\autoref{ProjMotion}. It is limited how useful learning is in this project, as
it is limited how much time and data we can provide for it to train with.

\subsubsection{Computational Limits}
An agent may not always be able to compute the best action to a problem quickly
enough, either due to memory limitation or just the sheer size of the problem.
This means that the agent may need to trade off the time it needs to compute a
solution with how good the solution is. This limit determines whether the agent
has \citep[ch.1.5.8]{MIBook}:

\begin{itemize}
  \item \textbf{Perfect rationality} - The agent tries to find the best action
  allways.
  \item \textbf{Bounded rationality} - The agent decides the best action taking
  its limitation into account.
\end{itemize}

\name has bounded rationality due to the fact that the NXT provides limited
resources, as well as the limited time the target is in range. This means that
in some situations, like deciding when to shoot, it will have to determine the
best course of action at the moment instead of spending a lot of time computing
the best solution. In addition to just the calculations, \name also needs to
move into a proper position, and the shot itself has a certain travel time. All
of this leads to a system which uses bounded rationality.

\subsection{Belief Network}\KT
\name is only is able to see the
target and determine the distance between them, it does not know where the
target is going nor how fast it is. This can lead to alot of different possible
situations, as it will need to try and determine the speed and direction through
additional observations of the distance. A belief network is a way to model
this conditional dependency. It is an acyclic directed graph, consisting of a
set of random variables functioning as nodes. These variables represent the
various features the model has, usually the based on the features of
the representation scheme. Nodes can also be created to act as
intermediary variables, which are created in order to either simplify the model
or to make the conditional dependency more logical. These variables affect each
other, this is modelled by the direction of the arc going between the nodes,
such as the targets position being affected by its speed. The variables each
have a set of the values as well as a set of conditional probability
distributions for each variable given its parents. \citep[ch.6.3]{MIBook}.
The variables make up the belief network and are connected via either a
\textbf{serial}, \textbf{convergent} or \textbf{divergent} connection. These
connections dictate how information is able flow through the network given
evidence on a node. Evidence occurs when a specific value in a node is observed,
which allows the probability of that value to be set to 1, while the others in
that node are set to 0.
\begin{itemize}
  \item A \textbf{serial} connection is a set of nodes connected in a sequence.
  This kind of connection represents that each node is affected by the node
  prior to it. These connections allow information to flow as long as the middle
  node is not blocked by evidence. If evidence is present on the middle node,
  \textit{Node2}, it is becomes impossible to say anything new about
  \textit{Node1} given more evidence further down the sequence. An example of a
  serial connection can be seen in \autoref{Serial2}.
 \figx[0.8]{Serial2}{An example of a serial connection}
  \item A \textbf{convergent} connection consists of a child node with two or
  more parents. This kind of connection represents that one node is affected by
  more than one node. In this connection the absence of evidence on the child
  node, \textit{Node3}, or one of its descendants makes it impossible to
  transmit information between the parent nodes. If however evidence is present
  on \textit{Node3}, or one of its descendants, it is possible to transmit
  evidence between the parent nodes. An example of a convergent connection can
  be seen in \autoref{Convergent2}.
 \figx[0.8]{Convergent2}{An example of convergent connection}
  \item A \textbf{divergent} connection is a parent node connected to two or
  more children. This kind of connection represents that one node affects more
  than two or more nodes. In this kind of connection it is only possible to
  transmit information between the children if the parent node, \textit{Node1}
  has evidence. An example of a divergent connection can be seen in
  \autoref{Divergent2}.
 \figx[0.8]{Divergent2}{An example of a divergent connection}
\end{itemize}



\subsection{Hidden Markov Model}

A belief network can be made to model a dynamic system by using a \textbf{Markov
chain} to represent a feature at different points in time as random
variables. By adding random variables called observation to the
Markov chain, it becomes a \textbf{hidden Markov model (HMM)}. This is done to
model how multiple observations allow for a prediction
\citep[ch.6.5.2]{MIBook}, see \autoref{HMM}.
\figx[0.9]{HMM}{A belief network as a hidden Markov model}

This way of modelling a system is aided by the independence assumption
\citep[p.240]{MIBook}, which states:
 
 \begin{center}
 \begin{minipage}{0.8\linewidth}
 \textit{Each random variable is conditionally independent of its non-descendants
 given its parents.}
 \end{minipage}
 \end{center}
 
For a Markov chain it means that when we want to consider a variable at time
\textit{t}, we only need to also consider the previous variable at time
\textit{t-1}. The same principle applies to an HMM, if the node has few parents,
the number of variables that it directly affect it are few.\nl

A Markov chain and by extension an HMM, can be extendeded indefinitely, meaning
that a small number of parameters can provide an infinite network
\citep[ch.6.5.1]{MIBook}. For example: A man observes an airplane taking off
northbound therefore adding evidence at random variable $O_0$, then at $O_1$ he
observes that the airplane has turned eastward, at $O_2$ he observes that it
has turned southward and at $O_3$ he sees that it continues south. This allows
him to guess the position of the plane at $P_4$, see \autoref{HMM}. 



% \subsection{Conclusion}
%  All of this has an influence on how the belief network for \name should be
%  structured, as the nodes in it is the velocity of the target, the position of
%  the target, the distance to it and the angle of fire. The information we get is
%  from the sensors which means that the evidene should be on the distance and
%  angle node. We want the evidence to flow in such a way that it is possible to
%  transmit evidence to the velocity node.\nl


% 
%  The reason for using a belief network is that it makes it possible to model the
%  world with the data that \name gets from it's sensors. This model can be
%  relatively compact Which is an advantage since the NXT does not have a lot of
%  processing power. Another reason for Using a belief network is that the \name
%  is only able to observe the world partially since there is some noise on the
%  sensors, which means that it needs to be able to reason under uncertainty.
%  A belief network is also useful since \name needs to be able to plan
%  ahead when taking actions, since it should be able to consider the movement of
%  the target before shooting.
