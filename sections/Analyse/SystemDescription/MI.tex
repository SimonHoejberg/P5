\section{Intelligent Agents}

As \name is an autonomous turret and needs to be able to hit a moving target,
it needs to have some intelligence in order to determine how best to hit the
target. The turret can be considered intelligent when its decisions
are explainable by computations while its intelligence can be judged
by\cite{MIBook}:

\begin{enumerate}
  \item If what it does is appropriate to its circumstances and goals.
  \item It is flexible to changing environments and changing goals.
  \item It learns from experience.
  \item It makes appropriate choices given its perceptual and computational
  limits.
\end{enumerate}
%See MI book, chapter 1.1 p.4

In order to be intelligent \name needs to be able to represent its knowledge in
a suitable way that allows it to work it into a suitable solution.

\subsection{Knowledge Representation}\label{KR}
%See MI book chapter 1.4 p.12

As \name is a turret it needs to be able to convert the problem of hitting a
moving target into a suitable representation. The representation can then be
computed into a suitable output, which can be interpreted as a solution.

%(Consider using fig1.4 p12)

The representation of a peice of knowledge is the internal representation of the
knowledge, and is part of the knowledge base, which is all the knowledge stored
in the agent. The form of this knowledge is represented in a representation
scheme\cite{MIBook}. An example of the knowledge that \name should have is 
knowledge about the movement of the target, the vertical angle of the canon and
the horisontal angle of the turret in a way that makes it possible to compute an
output. Whether this output is a solution or not, depends on what a sufficient
solution is. Generally there are 4 different categories of solutions:

\begin{itemize}
  \item Optimal solution - The best possible solution to the problem.
  \item Satisficing solution - A solution that is both satisfying and
  sufficient.
  \item Approximately optimal solution - A solution that is close to the optimal
  solution.
  \item Probable solution - A solution that is likely to be a solution but is
  not guaranteed to be.
\end{itemize}

As \name is shooting on a moving target. The solution needs to be reliable able
to hit the target, but as it is a real-time system there are certain limits to
how much time can be spent searching and waiting for the optimal shot.
\fix{Do we need to discuss which solution type we use?}{}


\subsection{Design Space}
The design space is defined by a series of 8 factors, which are used to give an
estimation of the system. These factors are:

\begin{itemize}
   \item Modularity
   \item Representation scheme
   \item Planning horizon
   \item Uncertainty
   \item Preference
   \item Number of agents
   \item Learning
   \item Computational limits
\end{itemize}

\subsubsection{Modularity}
Modularity describes how easily the system can be divided into modules, as well
as how easy it is to understand these modules separately. Modularity has three
different levels, flat, modular and hierachical. Flat means that everything is
contained within a single module. Modular means that the system can be divided
into several modules that can be understood separately from the system.
Hierachical means that the modules can recursively have additional modules
inside them. \name as an autonomous turret needs to have several separate
functions that allows for it turn, aim and determine the position of a target.
This means that \name should be somewhat modular due to the fact that it has
different tasks that can each be a module of its own.

\subsubsection{Representation Scheme}
As mentioned in \autoref{KR} the representation scheme is how knowledge is
represented. This knowledge is usually represented in terms of states which are
combination of all the relevant values. This leads to alot of different states
as the amount of possible values increases. Instead of reasoning using states it
is possible to combine the relevant values into feature. This means that the
state is determined by the value of the features and not the other way around.

\subsubsection{Planning Horizon}
The planning horizon of an intelligent agent is the time it considers the
consequenses of its actions. For example a child might only do something if
there is an immediate, whereas an adult might have the foresight to do something
for a reward thats some time off. There exist the following types of planning
horizons:
\begin{itemize}
	\item Non-planning - The agent does not consider the future when making
	decisions, or time is not involved as a factor.
	\item Finite horizon - The agent considers the future for a fixed finite set
	of steps. 
	\item Indefinite horizon - The agent looks ahead for finite but not fixed
	number of steps.
	\item Infinite horizon - The agent should run forever.	
\end{itemize}

Considering \name is an autonomous turret, we can already rule out the
non-planning horizon, since it will have to consider the future in order to
make a decision. Additionally the turret is an embedded system with limited
computational power and memory, an infinite horizon is thus not a realistic
option since the resulting statespace would require far too much computational
power.

\subsubsection{Uncertainty}

There are two parts to uncertainty. The first part is sensing uncertainty while
the second is effect uncertainty. Sensing uncertainty concerns what the agent
can determine based on its observations. There are two possibilities:
\begin{itemize}
  \item Fully observable - The agent can determine the state based on
  the observations.
  \item Partially observable - The agent can not directly observe the state.
  This can be caused by noisy observations or when multiple states result in the
  same observations.
\end{itemize}
In order have knowledge of the target, \name will need to observe it through
sensors. Since the properties of the target are not known by \name beforehand,
it will need to observe the target and calculate the properties. This results
in uncertainty as we not only need to predict the position of the target, the
sensors may receive noisy observations. We can thus already determine that \name
will only be able to partially observe the state.\nl

Effect uncertainty concerns how accuratly the agent can determine the resulting
state based on an action in its current state. There are two possibilities:
\begin{itemize}
  \item Deterministic - The resulting state can be determined based on an action
  in the current state.
  \item Stochastic - The resulting state is the result of a probability
  distribution.
\end{itemize}

While \name needs to hit a moving target and this produces some



Once the properties of the target are predicted, \name should have a
deterministic way of predicting the targets future positions.

For \name there are
While \name needs to hit a moving target and this pre 
\fix{needs some more thought}{}
 
\subsubsection{Preferences}
For an agent to be able to make good decisions it is necessary to provide an
idea of the goals we are trying to accomplish. Furthermore there are certain
preferences concerning the trade-off between different solutions.

\begin{itemize}
  \item Goals - These can either be considered achievement goals or
  maintenance goals. Maintenance goals must be achieved in all the states
  it visits. Achievement goals should hold true for the final state.
  \item Complex preferences - Allows us to describe and model more complex
  features such as desirability. In this category there are 2 distinct types.
  \textbf{Ordinal preferences} describe preferences where only the order of the
  preferences indicate importance. \textbf{Cardinal preferences} are those that
  are prioritized according to their value.
\end{itemize}

For this project, first and foremost an achievement goal is needed in order to
describe the goal for the agent. This is simply the combined state of features
that allows us to hit a target in motion. The complex preferences of the
project is limited by the scope of the project. For example one could imagine
the prioritizing of different targets according to a complex preference

\subsubsection{Number of Agents}

\subsubsection{Belief Network}
A belief network is an acyclic directed graph consisting of random variables and
a set of conditional probalility distributions. This an be used to model a
dynamic system by treating a feature at a given time as a random variable, which
is the idea behind Markov chains. A markov chain is a special belief network
that represents sequences of values.
In a markov chain at state $S_t$ the future is conditionally independent from
the past given the present. This gives that if the transition probabilities are
the same for each time point. To specify a Markov chain two conditional
probabilities must be specified:
\begin{itemize}
  \item $p(S_0)$ - species the initial condition
  \item $P(S_{t+1}|S_t)$ - specifies the dynamics, which is the same for each $t
  \geq 0$
\end{itemize}

These kinds of chains are of interrest due to the fact that the stationary
assumption is a natural model because the dynamics of the world ussually does
not change. The network can also extend indefinetly which means that with a
small number of parameters the network can be infinite.\nl

The Markov chain can be augmented to include observations which makes it a
Hidden Markov model. Like the state transitions from a Markov chain the Hidden
Markov model also includes observations about the states. If more than one state
maps to the same observation it is partial and if the same state maps to
different observations at different times it is noisy. The observations are
modeled using $O_t$ for each time t whose domain is the set of possible
observations. A Hidden Markov model needs three conditional proballity
distributions
\begin{itemize}
  \item $P(S_0)$ - species the initial condition
  \item $P(S_{t+1}|S_t)$ - specifies the dynamics, which is the same for each $t
  \geq 0$
  \item $P(O_t|S_t)$ - specifies the sensor model
\end{itemize}

This model makes it possible to determine the current state based on previous
and current observations. This is useful when tracking the target in order to
adjust \namep accordingly. This is done by having a belief network with 3
variables $Loc_i$, $Obs_i$ and $Act_i$ see \autoref{BeliefNetwork}. $Loc_i$ for
each time i represents \name's poition at time i, $Obs_i$ for each time i reprensts the observation
made at time i and for each time i $act_i$ represents the action taken at time
i. The model assumes the that at time i \namep is in position $Loc_i$, it
observes $Obs_i$ and acts, it observes $Act_i$ and time progresses to i+1 where
\namep is in position $Loc_{i+1}$. This means that at any given time t the
observation is only dependent on the state and at time t+1 the position is
dependent on the position and action at time t. This gives that the position
at time t+1 is conditionally independent from previous positions, actions and
observations given the position and ation at time t. 

\figx{BeliefNetwork}{A belief network for position of \name}


\subsubsection{Learning}
There are two ways of handling knowledge: knowledge is given and knwledge is
learned. If knowledge is given the agent does has all the information it needs
from the get-go.\nl 

when it comes to learning it is the ability to improve
behavior based on experience. A way to do this is with supervised learning where
a set of input features and target features are given together with a set of
traing examples, where the values for both input and output are given and a set
of test examples where only input is given. The goal is to predict the target
features for the test set as future examples.


\subsubsection{Computational Limits}
An angent may not always be able to cumpute the best action to a problem fast
enough. This is due to memory limitations that prevent it from from computing
fast enough. This means that the agent must trade off the time to compute the
solution with how good the solution is. This limit determines wether the agent
has:


\begin{itemize}
  \item Perfect rationality - The agent tries to find the best action without
  taking into account the limited resources.
  \item Bounded rationality - the agent decides the best ation given the
  computational limits.
\end{itemize}

\namep has bounded rationality due to the fat that the nxt does not have
unlimited resources. This means that in some situations, like deciding when
to shoot, it will have to determine the best course of action in the moment
instead of spending a lot of time computing the best solution. 
